{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UpT_ntgkou4o8f0ZnDt937cwj3bjyEh-","timestamp":1711566189408},{"file_id":"1Fu-LfAAQHDtyQW71thW00Q_uMzu2EJ3-","timestamp":1711556683997},{"file_id":"1GoiweQn3O4qoh3-cRgczoHlfbewZrTix","timestamp":1711393403200},{"file_id":"1rGZvDObG_RhLmRJWa7DK6A3kSQi1aVEr","timestamp":1711391693442},{"file_id":"1FzzwYjhSEp-nXtI2l14lXXA7Di5ofYpI","timestamp":1710279254307},{"file_id":"1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV","timestamp":1690472780450}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["**#CONCLUSIONES Y MODELO SELECCIONADO**"],"metadata":{"id":"XPlIc_vJiOfz"}},{"cell_type":"markdown","source":["1. Para el modelo Predictivo he usado como bag-of-words un TF-IDF\n","   consiguiendo como resultados:\n","\n","   Accuracy TRAIN: 0.8888518024032043\n","\n","   Accuracy TEST: 0.7837837837837838\n","\n","2. Para el Gradient Boosting he utilizado CountVectorizer como  bag-of-words en este caso los resultados:\n","\n","   Accuracy TRAIN: 0.8054072096128171\n","\n","   Accuracy TEST: = 0.8008008008008008\n","\n","3. Para el LSTM hemos obtenido como resultados:\n","\n","   Accuracy TRAIN: 0.751335084438324\n","\n","   Accuracy TEST: 0.7057057023048401\n","\n","El modelo seleccionado sería el GradientBoosting el motivo es que para test es el que mejor precisión tiene, el modelo de regresión no tiene una precisión mucho pero en test sin embargo, hay diferencia entre los resultados de train y test y esto podría indicar algo de overfitting, cosa que no está pasando en el Gradientboosting\n","\n","La elección del modelo, está basada en los resultados que he obtenido, pero realmente el volumen del dataset era muy bajo y me ha faltado tiempo para hacer más pruebas y cambios, por ejemplo, he usado bag-of-words diferentes pero el motivo ha sido que quería implementar los 2 en la práctica para entenderlos y quizás tendría que haber usado el mismo o ejecutarlos con ambos para ver diferencias.\n","\n","En un primer momento también opté por no lematizar, pero viendo resultados de N gramas y revisando revieuws añadí la lematización en el preprocesado, pero al volver a lanzar los modelos empeoraron los resultados, no sólo en cuanto a precisión, sino que la matriz de confusión empeoró pasando de clasificar bien la clase 0 en 405 casos a 397 y de 108 clasificaciones erróneas para la clase 1 pasó a 115.\n","Me hubiera gustado probar lematizando sólo los verbos por ejemplo, también me hubiera gustado hacer un análisis más profundo de las palabras porque, he visto que hay muchas relacionadas con la edad (years,  yr, old,etc) que no he quitado porque con ellas vemos que la edad es un factor importante a la hora de seleccionar un juego y de dejar una reseña, pero quizás no es necesario dejar todas las palabras relacionadas para así poder dar más importancia a otras que revelen otros razones importantes a la hora de dejar una reseña."],"metadata":{"id":"tj9wwC-AiU--"}},{"cell_type":"markdown","source":["#**EDA Y PREPROCESAMIENTO COMENTARIOS**\n","\n","Para la descarga había intentado quedarme con datos de un producto concreto (ASIN) y mediante un sample quedarme con mismo número de reseñas para cada overall, pero para ver si un producto podía tener al menos 1000 reseñas de cada uno, necesitaba cargar demasiados datos y me quedaba sin RAM, por lo que otra cosa que probé fué cargar menos reseñas y ver que productos tenian menos varianza entre los diferentes overall y de esa manera cargar las review del mismo producto y luego hacer el sample por overall, pero finalmente, por tiempo y por rendimiento de colab, opté por hacerlo de manera mas sencilla, no tener en cuenta el producto y quedarme con pocos registros que me permitieran desarrollar bien los ejercicios posteriores.\n","\n","Resultados comprobación de productos con menos varianza:\n","\n","ASIN: B00000ISC5, Varianza: 307216.55999999994\n","\n","ASIN: B00000IZJB, Varianza: 426037.\n","\n","ASIN: B00000J0S3, Varianza: 480487.76000000007\n","\n","ASIN: B00000IVAK, Varianza: 504221.43999999994\n","\n","ASIN: 157982319X, Varianza: 539754.24\n","\n","ASIN: 1932188126, Varianza: 540130.24\n","\n","\n","\n"],"metadata":{"id":"t74wF522TYXS"}},{"cell_type":"markdown","source":["#**MODELOS COMENTARIOS**\n","\n","Como explicaba en las conclusiones, he podido realizar algunas pruebas pero no todas las que me hubiera gustado,por ejemplo, he ejecutado también por curiosidad el TF-IDF con N grama de 2 porque el resultado durante el EDA me había parecido interesante, pero sobre todo me habría gustado poder trabajar con un conjunto de datos mucho mayor para poder ver realmente la diferencia entre los modelos de ML y el LSTM o un RNN, y tener más tiempo para,realizar un análisis másprofundo de los textos y poder ejectutar los modelos 2 veces para ver las diferencias también entre usar TF-IDF y CountVectorizer.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"hUJvQn_B3bTh"}}]}